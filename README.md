# Optimal-handling-of-Coinbase-dataset
Pooling different cryptocurrency data has led to our large dataset that is incrementally loaded into a Relational database every minute. Our data set marks the first concentrated effort toward high-fidelity panel data of crypto currency development for a wide range of metrics. We collected metrics like unix, date, symbol, open, close, high, low, timestamp, year, month volume and volume_fiat. This OHLC (Open/High/Low/Close) pricing data is updated on a minute basis .We have designed our database schema to support storage of our large and ongoing data. Through the course of this project we optimized the process of loading the data into the relational database by chunking, partitioning, addressing many small reads with indexing and replicating for failure recovery, data locality for both pure Database Connectivity and Object Relational Model.
